{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Regression Modeling Workflow\n",
    "\n",
    "This dataset was downloaded from [Kaggle](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho) and contains information about used car sale listings.  We are trying to predict the price associated with the listing.\n",
    "\n",
    "### Features (as described on Kaggle)\n",
    " - `Car_Name`: The name of the car\n",
    " - `Year`: The year in which the car was bought\n",
    " - `Selling_Price`: The price the owner wants to sell the car at\n",
    " - `Present_Price`: The current ex-showroom price of the car\n",
    " - `Kms_Driven`: The distance completed by the car in km\n",
    " - `Fuel_Type`: The fuel type of the car (`Petrol`, `Diesel`, or Other)\n",
    " - `Seller_Type`: Whether the seller is a dealer or an individual\n",
    " - `Transmission`: Whether the car is manual or automatic\n",
    " - `Owner`: The number of owners the car has previously had\n",
    "\n",
    "Looking at the original website, it looks like the prices are listed in lakhs, meaning hundreds of thousands of rupees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Before performing any preprocessing or modeling, set aside a holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Selling_Price\", axis=1)\n",
    "y = df[\"Selling_Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Linear Regression with Numeric Features Only\n",
    "\n",
    "We have four numeric features (`Year`, `Present_Price`, `Kms_Driven`, and `Owner`) and four non-numeric features (`Car_Name`, `Fuel_Type`, `Seller_Type`, `Transmission`).  Before doing any of the engineering work to be able to use those non-numeric features, let's just try using the numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "X_train_numeric = X_train[[\"Year\", \"Present_Price\", \"Kms_Driven\", \"Owner\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cross_val_score = cross_val_score(lin_reg_model, X_train_numeric, y_train)\n",
    "baseline_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad, we are getting somewhere between 0.67 and 0.89 r-squared for a linear regression with just the numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add One-Hot Encoded Features\n",
    "\n",
    "Let's see if adding in some of those non-numeric features helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index so we can concatenate the one-hot encoded dfs more easily\n",
    "X_train_all_features = X_train.copy().reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_concat_feature_train(X_train_all_features, feature_name):\n",
    "    \"\"\"\n",
    "    Helper function for transforming training data.  It takes in the full X dataframe and\n",
    "    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n",
    "    with that feature transformed into multiple columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # make a one-hot encoder and fit it to the training data\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    single_feature_df = X_train_all_features[[feature_name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    \n",
    "    # call helper function that actually encodes the feature and concats it\n",
    "    X_train_all_features = encode_and_concat_feature(X_train_all_features, feature_name, ohe)\n",
    "    \n",
    "    return ohe, X_train_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_concat_feature(X, feature_name, ohe):\n",
    "    \"\"\"\n",
    "    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n",
    "    in both training and testing steps.  Takes in the full X dataframe, feature name, \n",
    "    and encoder, and returns the dataframe with that feature transformed into multiple\n",
    "    columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # create new one-hot encoded df based on the feature\n",
    "    single_feature_df = X[[feature_name]]\n",
    "    feature_array = ohe.transform(single_feature_df).toarray()\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0])\n",
    "    \n",
    "    # drop the old feature from X and concat the new one-hot encoded df\n",
    "    X = X.drop(feature_name, axis=1)\n",
    "    X = pd.concat([X, ohe_df], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need each of these encoders later for transforming the test data\n",
    "\n",
    "fuel_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Fuel_Type\")\n",
    "seller_type_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Seller_Type\")\n",
    "transmission_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Transmission\")\n",
    "# putting car name at the end just because there are the most categories\n",
    "car_name_ohe, X_train_all_features = encode_and_concat_feature_train(X_train_all_features, \"Car_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with More Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "print(\"Old:\", baseline_cross_val_score)\n",
    "print(\"New:\", cross_val_score(lin_reg_model, X_train_all_features, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks worse.  What if we don't use the car name, and just use the categories with 1-3 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_except_car_name = X_train_all_features[[\n",
    "                    \"Year\",\n",
    "                    \"Present_Price\",\n",
    "                    \"Kms_Driven\",\n",
    "                    \"Owner\",\n",
    "                    \"CNG\",\n",
    "                    \"Diesel\",\n",
    "                    \"Petrol\",\n",
    "                    \"Dealer\",\n",
    "                    \"Individual\",\n",
    "                    \"Automatic\",\n",
    "                    \"Manual\"\n",
    "                ]].copy()\n",
    "X_train_all_except_car_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "print(\"Old:\", baseline_cross_val_score)\n",
    "print(\"New:\", cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, adding these categories improved r-squared for 4 out of 5 subsamples compared to just having numeric features, so let's keep them for our linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_linreg_cross_val_score = cross_val_score(lin_reg_model, X_train_all_except_car_name, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a More Advanced Model\n",
    "\n",
    "It depends on our business case whether these numbers are sufficient.  We are explaining approximately somewhere between 65% and 92% of the variance in the sale price.  But let's try a more complicated model.\n",
    "\n",
    "First, just using the X_train values used in the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_1 = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "print(\"Old:\", best_linreg_cross_val_score)\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this more-sophisticated model is performing slightly better on 4 of 5 subsamples than the best linear regression score.  Let's see what happens if we add the car names back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_2 = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_2, X_train_all_features, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one of the subsamples improved with adding this feature, and everything else got worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning the More Advanced Model\n",
    "\n",
    "Let's add some more \"power\" to the random forest regressor, since it's running reasonably quickly right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_1, X_train_all_except_car_name, y_train))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That marginally improved 4 of the 5 subsamples (but was significantly slower to run).  Let's try including the car name again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "print(\"Old:\", cross_val_score(random_forest_regressor_model_3, X_train_all_except_car_name, y_train))\n",
    "print(\"New:\", cross_val_score(random_forest_regressor_model_4, X_train_all_features, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, that didn't really seem to help.  So if we're stopping right now, we can say that the third random forest regressor is the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we have chosen a best model, let's use the holdout set to see how well the final model does\n",
    "\n",
    "First, perform all of the same transformations on the test X that were performed on the train X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all_except_car_name = X_test.reset_index().drop([\"index\", \"Car_Name\"], axis=1)\n",
    "\n",
    "# fuel_type_ohe, seller_type_ohe, and transmission_ohe were fitted on the training data\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Fuel_Type\", fuel_type_ohe)\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Seller_Type\", seller_type_ohe)\n",
    "X_test_all_except_car_name = encode_and_concat_feature(X_test_all_except_car_name, \"Transmission\", transmission_ohe)\n",
    "\n",
    "X_test_all_except_car_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit our best model on all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.fit(X_train_all_except_car_name, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score our best model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_regressor_model_3.score(X_test_all_except_car_name, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good!  We have a model that is able to explain 97% of the variance in the car sale list prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (prework-labs)",
   "language": "python",
   "name": "prework-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
